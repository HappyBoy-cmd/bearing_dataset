{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "toc-hr-collapsed": false
   },
   "source": [
    "# Introduction to dataset\n",
    "The experiment was designed and completed in 2014 by Christian Lessmeier, Enge-Rosenblatt, Bayer, & Zimmer, University of Paderborn, Germany.\n",
    "\n",
    "Data download link: https://mb.uni-paderborn.de/kat/forschung/datacenter/bearing-datacenter/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Data Sets and Download\n",
    "\n",
    "The main characteristic of the data availible from the Paderborn university are\n",
    "\n",
    "Synchronously measured motor currents and vibration signals with high resolution and sampling rate of __26 damaged bearing states__ and __6 undamaged (healthy) states__ for reference.\n",
    "\n",
    "Supportive measurement of speed, torque, radial load, and temperature.\n",
    "\n",
    "Four different operating conditions (see operating conditions).\n",
    "20 measurements of 4 seconds each for each setting, saved as a MatLab file with a name consisting of the code of the operating condition and the four-digit bearing code (e.g. N15_M07_F10_KA01_1.mat).\n",
    "\n",
    "Systematic description of the bearing damage by uniform fact sheets and a measuring log, which can be downloaded with the data.\n",
    "\n",
    "In total, experiments with 32 different bearing damages in ball bearings of type 6203 were performed:\n",
    "* Undamaged (healthy) bearings (6x), see Table 6 in (pdf).\n",
    "* Artificially damaged bearings (12x), see Table 4 in (pdf).\n",
    "* Bearings with real damages caused by accelerated lifetime tests, (14x) see Table 5 in (pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Dataset description & organisation\n",
    "each experiments data is stored at the universitys homepage, labelled with the __Bearing Code__ (from the table below).__rar__ further below a number of utility finctions for retrieving det data is implemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Artificial damages\n",
    "The dataset is composed of artificially created and wear based bearing errors.\n",
    "\n",
    "<img src=\"doc/ad.png\" width=\"500\" height=\"250\">\n",
    "\n",
    "The artifiaical damages are manufactured using three different methods:\n",
    "1. electric discharge machining, **EDM** (trench of 0.25 mm length\n",
    "in rolling direction and depth of 1-2 mm),\n",
    "2. drilling (diameter: 0.9 mm, 2 mm, 3 mm), and\n",
    "3. manual electric engraving (damage length from\n",
    "1-4 mm) \n",
    "\n",
    "ISO 15243 gives a methodology for the classification of\n",
    "bearing damage and failures. The damages are categorized\n",
    "into six main damage modes and their sub-modes. The six\n",
    "main damage modes are: fatigue, wear, corrosion, electrical\n",
    "erosion, plastic deformation, and fracture and cracking.\n",
    "\n",
    "**Damage extent for a 6203 bearing**\n",
    "\n",
    "|Damage level | Assigned % value | Limites for 6203 bearing |\n",
    "|--- | --- | --- |\n",
    "| 1 |0-2%|<2mm|\n",
    "| 2 |2-5%|>2mm|\n",
    "| 3 |5-15%|>4.5mm| \n",
    "| 4 |15-35%|>13.5mm|\n",
    "| 5 |>35%|>31.5mm|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Real Bearing Damage Samples by Accelerated Lifetime Tests\n",
    "ball bearings with real\n",
    "damages were obtained from an accelerated life time test.\n",
    "The accelerated life time test rig consists of a bearing\n",
    "housing and an electric motor, which powers a shaft with\n",
    "four test bearings of type 6203 in the housing (Figure).\n",
    "\n",
    "<img src=\"doc/fig2.png\" width=\"400\" height=\"250\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Dataset summary \n",
    "In the tavles below a summary of the availble dataset are summarised\n",
    "\n",
    "The availible datasets contains following subsets of data, availivble for download, by the name provided in the initial column\n",
    "\n",
    "### Healthy bearings\n",
    "|Bearing Code | Run-in Period [h] | Radial Load [N] | Speed ​​[min ^ -1] |\n",
    "|--- | --- | --- | --- |\n",
    "| K001 |> 50 | 1000-3000 | 1500-2000 |\n",
    "| K002 | 19 | 3000 | 2900 |\n",
    "| K003 | 1 | 3000 | 3000 | \n",
    "| K004 | 5 | 3000 | 3000 | |\n",
    "| K005 | 10 | 3000 | 3000 | |\n",
    "| K006 | 16 | 3000 | 2900 | |\n",
    "\n",
    "### Bearings with outer ring damage\n",
    "|Bearing Code | Radial Load [N] | Speed ​​[min ^ -1] | Damage method | Damage extent (level) | Damage type |\n",
    "|--- | --- | --- | --- | --- | --- |\n",
    "| KA01 | - | - | EDM |1||\n",
    "| KA03 | - | - | Elec. Engraved|2||\n",
    "| KA04 | - | - | HALT|1|Fatigue,pitting|\n",
    "| KA05 | - | - | Elec. Engraved|1|\n",
    "| KA06 | - | - | Elec. Engraved|2|\n",
    "| KA07 | - | - | Drilling |1|\n",
    "| KA08 | - | - | Drilling |2|\n",
    "| KA09 | - | - | Drilling |2|\n",
    "| KA15 | - |-|HALT|1|Plastic deform.:Indentations|\n",
    "| KA16 | - |-|HALT|2|fatigue: pitting|\n",
    "| KA22 | - |-|HALT|1|fatigue: pitting|\n",
    "| KA30 | - |-|HALT|1|Plastic deform.:Indentations|\n",
    "\n",
    "### Bearings with inner ring damage\n",
    "|Bearing Code | Radial Load [N] | Speed ​​[min ^ -1] | Damage method | Damage extent (level) | Damage type |\n",
    "|--- | --- | --- | --- | --- | --- |\n",
    "| KI01 | - | - |  EDM |1|\n",
    "| KI03 | - | - | Elec. Engraved|1|\n",
    "| KI04 | - | - | HALT |1|Fatigue,pitting|\n",
    "| KI05 | - | - | Elec. Engraved|1|\n",
    "| KI07 | - | - |  Elec. Engraved|2|\n",
    "| KI08 | - | - |  Elec. Engraved|2|\n",
    "| KI14 | - | - | HALT |1|Fatigue,pitting |\n",
    "| KI16 | - | - | HALT |3|Fatigue,pitting |\n",
    "| KI17 | - | - | HALT |1|Fatigue,pitting |\n",
    "| KI18 | - | - | HALT |2|Fatigue,pitting |\n",
    "| KI21 | - | - | HALT |1|Fatigue,pitting |\n",
    "\n",
    "### Bearings with combined inner and outer ring damage\n",
    "|Bearing Code | Radial Load [N] | Speed ​​[min ^ -1] | Damage method | Damage extent (level) |Damage type |\n",
    "|--- | --- | --- | --- | --- | --- |\n",
    "| KB23 | - | - | HALT |2|fatigue: pitting|\n",
    "| KB24 | - | - | HALT |3|fatigue: pitting|\n",
    "| KB27 | - | - | HALT |1|Plastic deform.:Indentations|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "'''\n",
    "## Analysis and dataset planning\n",
    "After implementation of analysis and feature extraction functions, the artificial and real datasets are organised and tested as follows\n",
    "\n",
    "|Class||Training|Testing|\n",
    "--- |--- |--- |--- |\n",
    "|1|Healthy|K002|K001|\n",
    "|2|OR damage||KA22|\n",
    "|2|OR damage|KA01|K004|\n",
    "|2|OR damage|KA05|KA15|\n",
    "|2|OR damage|KA07|KA30|\n",
    "|2|OR damage||KA16|\n",
    "|3|IR damage||KI14|\n",
    "|3|IR damage|KI01|KI21|\n",
    "|3|IR damage|KI05|KI17|\n",
    "|3|IR damage|KI07|KI18|\n",
    "|3|IR damage||K16|\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Code for retrieving datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Create data folders & add to ```.gitignore```\n",
    "\n",
    "No data is to be commited to github since it's not meant for hosting data, just code'\n",
    "\n",
    "Therefore we add ```data```directory to git ignore list of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# create a data folder if such doesnt exist\n",
    "!mkdir -p data\n",
    "\n",
    "#add the data folder to gitignore, to avoid commiting the data\n",
    "gi_path = '../../../../.gitignore'\n",
    "\n",
    "# this path, strip the part we need\n",
    "dir = !pwd\n",
    "dir = str(dir[0])\n",
    "s = dir.split('/')\n",
    "#s\n",
    "# build working dir string\n",
    "wd = ('\\n' + s[-4] + '/' + s[-3]+ '/' + s[-2]+ '/' + s[-1] + '/data')\n",
    "wd\n",
    "# open and append to file\n",
    "f = open(gi_path, \"a\")\n",
    "print(wd, file=f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Install missing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: patool in /Users/opprud/opt/anaconda3/lib/python3.7/site-packages (1.12)\n",
      "Updating Homebrew...\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "#!pip install rarfile\n",
    "!pip install patool\n",
    "!brew install unrar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Retrieve datasets and unpack\n",
    "The code below will  download a rar file of interest and return the **unpacked, unshuffled** datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import errno\n",
    "import random\n",
    "#import urllib\n",
    "import urllib.request as ug\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import patoolib as pa\n",
    "\n",
    "rarpath = \"/usr/local/bin/unrar\"\n",
    "\n",
    "class PDB:\n",
    "    def __init__(self, exp, rpm, rad_force, torque_mNm, length):\n",
    "        #    def __init__(self):\n",
    "        if exp not in ('K001', 'K002', 'K003', 'K004', 'K005', 'K006','KA01','KA02','KA03','KA04','KA05','KA06'\n",
    "                       ,'KA07','KA08','KA09','KA15','KA16','KA22','KA30','KB23','KB24','KB27','KI01','KI03','KI04'\n",
    "                       ,'KI05','KI07','KI08','KI14','KI16','KI17','KI18','KI21'):\n",
    "            print(\"wrong experiment name: {}\".format(exp))\n",
    "            exit(1)\n",
    "        if rpm not in ('1500', '900'):\n",
    "            print(\"wrong rpm value: {}\".format(rpm))\n",
    "            exit(1)\n",
    "        if rad_force not in ('1000', '400'):\n",
    "            print(\"wrong load value: {}\".format(rad_force))\n",
    "            exit(1)\n",
    "        if torque_mNm not in ('100', '700'):\n",
    "            print(\"wrong torque value: {}\".format(torque_mNm))\n",
    "            exit(1)\n",
    "\n",
    "        dict_rpm = {'1500': 'N15_', '900': 'N09_', '2900': 'N29_'}\n",
    "        dict_torq = {'100': 'M01_', '700': 'M07_'}\n",
    "        dict_load = {'400': 'F04_', '1000': 'F10_'}\n",
    "        #Labels 1 = healthy bearing, 2 = outer ring damage , 3 = inner ring damage, 4 = combined damage\n",
    "        dict_labels = {'K0': 1, 'KA': 2, 'KI': 3, 'KB': 4}\n",
    "\n",
    "        #print(exp[0:2])\n",
    "        self.y_label = dict_labels[exp[0:2]]\n",
    "        print(\"Y is:\")\n",
    "        print(self.y_label)\n",
    "\n",
    "        filestring = dict_rpm[rpm] + dict_torq[torque_mNm] + dict_load[rad_force]\n",
    "        print('filestring')\n",
    "        print(filestring)\n",
    "\n",
    "        # create reciveing dir names from arguments\n",
    "        rdir = os.path.join('.', 'data/PDB', exp)  # ,rpm,load)\n",
    "        fmeta = os.path.join(os.path.dirname('.'), 'metadata.txt')\n",
    "        all_lines = open(fmeta).readlines()\n",
    "\n",
    "        lines = []\n",
    "        for line in all_lines:\n",
    "            l = line.split()\n",
    "            # print(l)\n",
    "            if (l[0] == exp): # and l[1] == rpm and l[2] == rad_force:  # and l[3] == torque_mNm:\n",
    "                lines.append(l)\n",
    "\n",
    "        print(\"l: \")\n",
    "        print(lines)\n",
    "        # prepare download\n",
    "        self.length = length  # sequence length\n",
    "        #        self._load_and_slice_data2(rdir, lines)\n",
    "        self._load_and_slice_data(rdir, lines, filestring)\n",
    "\n",
    "        # shuffle training and test arrays\n",
    "        shuffle = 0\n",
    "        if(shuffle):\n",
    "            self._shuffle()\n",
    "\n",
    "    def _mkdir(self, path):\n",
    "        try:\n",
    "            os.makedirs(path)\n",
    "        except OSError as exc:\n",
    "            if exc.errno == errno.EEXIST and os.path.isdir(path):\n",
    "                pass\n",
    "            else:\n",
    "                print(\"can't create directory '{}''\".format(path))\n",
    "                exit(1)\n",
    "\n",
    "    def _download(self, fpath, link):\n",
    "        print(\"Downloading to: '{}'\".format(fpath))\n",
    "        #urllib.request.urlretrieve(link, fpath)\n",
    "        ug.urlretrieve(link, fpath)\n",
    "\n",
    "    def _load_and_slice_data(self, rdir, infos, filestring):\n",
    "\n",
    "        self.X_train = np.zeros((0, self.length))\n",
    "        self.X_test = np.zeros((0, self.length))\n",
    "        self.y_train = []\n",
    "        self.y_test = []\n",
    "\n",
    "        ## w\n",
    "        for idx, info in enumerate(infos):\n",
    "\n",
    "            # directory to put the raw rar file\n",
    "            rawdir = os.path.join(rdir, 'raw')\n",
    "            self._mkdir(rawdir)\n",
    "\n",
    "            # path to find the file\n",
    "            fpath = os.path.join(rawdir, info[0] + '.rar')\n",
    "\n",
    "            # if file already exists, avoid duplicate downloads\n",
    "            if not os.path.exists(fpath):\n",
    "                print(\"no dir/file\")\n",
    "                self._download(fpath, info[3].rstrip('\\n'))\n",
    "\n",
    "            # compressed file to uncompress\n",
    "            cmpfile = rawdir + '/' + info[0] + '.rar'\n",
    "\n",
    "            print(\"file to exrtract is is::\")\n",
    "            print(cmpfile)\n",
    "\n",
    "            # unpack file\n",
    "            if not os.path.exists(rdir + '/' + info[0]):\n",
    "                pa.extract_archive(cmpfile, outdir=rdir, program=rarpath)\n",
    "            else:\n",
    "                print(\"file already extracted, skipping unrar\")\n",
    "\n",
    "            # a list of all files in the extracted dir\n",
    "            ddir = rdir + '/' + info[0]\n",
    "            flist_all = os.listdir(ddir)\n",
    "\n",
    "            # print(\"filelist:\")\n",
    "            # print(flist_all)\n",
    "\n",
    "            # use the searchstring, build from the program arguments to find files of interest\n",
    "            flistsorted = [i for i in flist_all if filestring in i]\n",
    "\n",
    "            print(\"sorted filelist:\")\n",
    "            print(flistsorted)\n",
    "\n",
    "            # now build the dataset from all files of interest\n",
    "            # iterate through the filelist\n",
    "            for f in flistsorted:\n",
    "                # load matlab file\n",
    "                mat_dict = loadmat(ddir + '/' + f)#,struct_as_record=False)\n",
    "\n",
    "                # get the values key, tha name of thenactual dataset equal to filename\n",
    "                #key = list(filter(lambda x: 'N15_M07_F04_' in x, mat_dict.keys()))\n",
    "                key = list(filter(lambda x: filestring in x, mat_dict.keys()))\n",
    "                # load data\n",
    "                #time_series = mat_dict[key[0]][:, 0] #['Y']\n",
    "                time_series = mat_dict[key[0]]['Y'][0, 0][0, 6][2][:][0]\n",
    "\n",
    "                idx_last = -(time_series.shape[0] % self.length)\n",
    "                clips = time_series[:idx_last].reshape(-1, self.length)\n",
    "\n",
    "                n = clips.shape[0]\n",
    "\n",
    "                split = 0\n",
    "                if(split):\n",
    "                    # 75% train 25%test\n",
    "                    n_split = int(3 * n / 4)\n",
    "\n",
    "                    self.X_train = np.vstack((self.X_train, clips[:n_split]))\n",
    "                    self.X_test = np.vstack((self.X_test, clips[n_split:]))\n",
    "                    # todo add meaning full label\n",
    "\n",
    "                    self.y_train += [self.y_label] * n_split\n",
    "                    self.y_test  += [self.y_label] * (clips.shape[0] - n_split)\n",
    "\n",
    "                else:\n",
    "                    self.X_train = np.vstack((self.X_train, clips[:n]))\n",
    "                    #self.X_test = np.vstack((self.X_test, clips[n_split:]))\n",
    "                    # todo add meaning full label\n",
    "\n",
    "                    self.y_train += [self.y_label] * n#_split\n",
    "                    #self.y_test  += [self.y_label] * (clips.shape[0] - n_split)\n",
    "                    \n",
    "    def _shuffle(self):\n",
    "        # shuffle training samples\n",
    "        index = list(range(self.X_train.shape[0]))\n",
    "        random.Random(0).shuffle(index)\n",
    "        self.X_train = self.X_train[index]\n",
    "        self.y_train = tuple(self.y_train[i] for i in index)\n",
    "\n",
    "        # shuffle test samples\n",
    "        index = list(range(self.X_test.shape[0]))\n",
    "        random.Random(0).shuffle(index)\n",
    "        self.X_test = self.X_test[index]\n",
    "        self.y_test = tuple(self.y_test[i] for i in index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Determine dataset shape\n",
    "In order to be able to detect innerring damage, we expect that at least one full revolution is rquired, for the inner ring to pass through the load zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2560"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The signal is digitalized and saved synchronously to the MCS with a sampling rate of 64 kHz.\n",
    "Fs = 64000\n",
    "# during the experiments 900 and 1500 RPM are used\n",
    "ExperimentRPM = 1500\n",
    "\n",
    "SamplePerRotation = int(Fs*(60/ExperimentRPM))\n",
    "SamplePerRotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "SliceLen = 500#SamplePerRotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let's take a look at the data\n",
    "the returned object returns an object including test and train data\n",
    "\n",
    "We want data from a full rotation in the bearing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Build a corpus of data for test and train from artificially made faults\n",
    "The used data is build from artificial test data as suggested i the accompanying article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y is:\n",
      "1\n",
      "filestring\n",
      "N15_M07_F10_\n",
      "l: \n",
      "[['K002', '1500', '400', 'http://groups.uni-paderborn.de/kat/BearingDataCenter/K002.rar']]\n",
      "file to exrtract is is::\n",
      "./data/PDB/K002/raw/K002.rar\n",
      "file already extracted, skipping unrar\n",
      "sorted filelist:\n",
      "['N15_M07_F10_K002_12.mat', 'N15_M07_F10_K002_13.mat', 'N15_M07_F10_K002_11.mat', 'N15_M07_F10_K002_10.mat', 'N15_M07_F10_K002_14.mat', 'N15_M07_F10_K002_15.mat', 'N15_M07_F10_K002_17.mat', 'N15_M07_F10_K002_8.mat', 'N15_M07_F10_K002_9.mat', 'N15_M07_F10_K002_16.mat', 'N15_M07_F10_K002_4.mat', 'N15_M07_F10_K002_5.mat', 'N15_M07_F10_K002_18.mat', 'N15_M07_F10_K002_7.mat', 'N15_M07_F10_K002_6.mat', 'N15_M07_F10_K002_19.mat', 'N15_M07_F10_K002_2.mat', 'N15_M07_F10_K002_3.mat', 'N15_M07_F10_K002_20.mat', 'N15_M07_F10_K002_1.mat']\n",
      "Y is:\n",
      "1\n",
      "filestring\n",
      "N15_M07_F10_\n",
      "l: \n",
      "[['K001', '1500', '400', 'http://groups.uni-paderborn.de/kat/BearingDataCenter/K001.rar']]\n",
      "file to exrtract is is::\n",
      "./data/PDB/K001/raw/K001.rar\n",
      "file already extracted, skipping unrar\n",
      "sorted filelist:\n",
      "['N15_M07_F10_K001_20.mat', 'N15_M07_F10_K001_19.mat', 'N15_M07_F10_K001_9.mat', 'N15_M07_F10_K001_8.mat', 'N15_M07_F10_K001_18.mat', 'N15_M07_F10_K001_15.mat', 'N15_M07_F10_K001_5.mat', 'N15_M07_F10_K001_4.mat', 'N15_M07_F10_K001_14.mat', 'N15_M07_F10_K001_16.mat', 'N15_M07_F10_K001_6.mat', 'N15_M07_F10_K001_7.mat', 'N15_M07_F10_K001_17.mat', 'N15_M07_F10_K001_3.mat', 'N15_M07_F10_K001_13.mat', 'N15_M07_F10_K001_12.mat', 'N15_M07_F10_K001_2.mat', 'N15_M07_F10_K001_10.mat', 'N15_M07_F10_K001_11.mat', 'N15_M07_F10_K001_1.mat']\n",
      "Y is:\n",
      "2\n",
      "filestring\n",
      "N15_M07_F10_\n",
      "l: \n",
      "[['KA22', '1500', '400', 'http://groups.uni-paderborn.de/kat/BearingDataCenter/KA22.rar']]\n",
      "file to exrtract is is::\n",
      "./data/PDB/KA22/raw/KA22.rar\n",
      "file already extracted, skipping unrar\n",
      "sorted filelist:\n",
      "['N15_M07_F10_KA22_9.mat', 'N15_M07_F10_KA22_8.mat', 'N15_M07_F10_KA22_20.mat', 'N15_M07_F10_KA22_19.mat', 'N15_M07_F10_KA22_18.mat', 'N15_M07_F10_KA22_16.mat', 'N15_M07_F10_KA22_17.mat', 'N15_M07_F10_KA22_15.mat', 'N15_M07_F10_KA22_14.mat', 'N15_M07_F10_KA22_10.mat', 'N15_M07_F10_KA22_11.mat', 'N15_M07_F10_KA22_13.mat', 'N15_M07_F10_KA22_12.mat', 'N15_M07_F10_KA22_5.mat', 'N15_M07_F10_KA22_4.mat', 'N15_M07_F10_KA22_6.mat', 'N15_M07_F10_KA22_7.mat', 'N15_M07_F10_KA22_3.mat', 'N15_M07_F10_KA22_2.mat', 'N15_M07_F10_KA22_1.mat']\n",
      "Y is:\n",
      "2\n",
      "filestring\n",
      "N15_M07_F10_\n",
      "l: \n",
      "[['KA01', '1500', '400', 'http://groups.uni-paderborn.de/kat/BearingDataCenter/KA01.rar']]\n",
      "file to exrtract is is::\n",
      "./data/PDB/KA01/raw/KA01.rar\n",
      "file already extracted, skipping unrar\n",
      "sorted filelist:\n",
      "['N15_M07_F10_KA01_8.mat', 'N15_M07_F10_KA01_17.mat', 'N15_M07_F10_KA01_16.mat', 'N15_M07_F10_KA01_9.mat', 'N15_M07_F10_KA01_14.mat', 'N15_M07_F10_KA01_15.mat', 'N15_M07_F10_KA01_11.mat', 'N15_M07_F10_KA01_10.mat', 'N15_M07_F10_KA01_12.mat', 'N15_M07_F10_KA01_13.mat', 'N15_M07_F10_KA01_1.mat', 'N15_M07_F10_KA01_2.mat', 'N15_M07_F10_KA01_20.mat', 'N15_M07_F10_KA01_3.mat', 'N15_M07_F10_KA01_7.mat', 'N15_M07_F10_KA01_18.mat', 'N15_M07_F10_KA01_19.mat', 'N15_M07_F10_KA01_6.mat', 'N15_M07_F10_KA01_4.mat', 'N15_M07_F10_KA01_5.mat']\n",
      "Y is:\n",
      "1\n",
      "filestring\n",
      "N15_M07_F10_\n",
      "l: \n",
      "[['K004', '1500', '400', 'http://groups.uni-paderborn.de/kat/BearingDataCenter/K004.rar']]\n",
      "file to exrtract is is::\n",
      "./data/PDB/K004/raw/K004.rar\n",
      "file already extracted, skipping unrar\n",
      "sorted filelist:\n",
      "['N15_M07_F10_K004_8.mat', 'N15_M07_F10_K004_9.mat', 'N15_M07_F10_K004_10.mat', 'N15_M07_F10_K004_11.mat', 'N15_M07_F10_K004_13.mat', 'N15_M07_F10_K004_12.mat', 'N15_M07_F10_K004_16.mat', 'N15_M07_F10_K004_17.mat', 'N15_M07_F10_K004_15.mat', 'N15_M07_F10_K004_14.mat', 'N15_M07_F10_K004_19.mat', 'N15_M07_F10_K004_18.mat', 'N15_M07_F10_K004_20.mat', 'N15_M07_F10_K004_2.mat', 'N15_M07_F10_K004_3.mat', 'N15_M07_F10_K004_1.mat', 'N15_M07_F10_K004_4.mat', 'N15_M07_F10_K004_5.mat', 'N15_M07_F10_K004_7.mat', 'N15_M07_F10_K004_6.mat']\n",
      "Y is:\n",
      "2\n",
      "filestring\n",
      "N15_M07_F10_\n",
      "l: \n",
      "[['KA05', '1500', '400', 'http://groups.uni-paderborn.de/kat/BearingDataCenter/KA05.rar']]\n",
      "file to exrtract is is::\n",
      "./data/PDB/KA05/raw/KA05.rar\n",
      "file already extracted, skipping unrar\n",
      "sorted filelist:\n",
      "['N15_M07_F10_KA05_10.mat', 'N15_M07_F10_KA05_11.mat', 'N15_M07_F10_KA05_13.mat', 'N15_M07_F10_KA05_12.mat', 'N15_M07_F10_KA05_16.mat', 'N15_M07_F10_KA05_17.mat', 'N15_M07_F10_KA05_15.mat', 'N15_M07_F10_KA05_14.mat', 'N15_M07_F10_KA05_9.mat', 'N15_M07_F10_KA05_8.mat', 'N15_M07_F10_KA05_5.mat', 'N15_M07_F10_KA05_4.mat', 'N15_M07_F10_KA05_6.mat', 'N15_M07_F10_KA05_7.mat', 'N15_M07_F10_KA05_3.mat', 'N15_M07_F10_KA05_2.mat', 'N15_M07_F10_KA05_1.mat', 'N15_M07_F10_KA05_19.mat', 'N15_M07_F10_KA05_18.mat', 'N15_M07_F10_KA05_20.mat']\n",
      "Y is:\n",
      "2\n",
      "filestring\n",
      "N15_M07_F10_\n",
      "l: \n",
      "[['KA15', '1500', '400', 'http://groups.uni-paderborn.de/kat/BearingDataCenter/KA15.rar']]\n",
      "file to exrtract is is::\n",
      "./data/PDB/KA15/raw/KA15.rar\n",
      "file already extracted, skipping unrar\n",
      "sorted filelist:\n",
      "['N15_M07_F10_KA15_7.mat', 'N15_M07_F10_KA15_15.mat', 'N15_M07_F10_KA15_14.mat', 'N15_M07_F10_KA15_6.mat', 'N15_M07_F10_KA15_4.mat', 'N15_M07_F10_KA15_16.mat', 'N15_M07_F10_KA15_17.mat', 'N15_M07_F10_KA15_5.mat', 'N15_M07_F10_KA15_1.mat', 'N15_M07_F10_KA15_13.mat', 'N15_M07_F10_KA15_12.mat', 'N15_M07_F10_KA15_2.mat', 'N15_M07_F10_KA15_10.mat', 'N15_M07_F10_KA15_11.mat', 'N15_M07_F10_KA15_3.mat', 'N15_M07_F10_KA15_20.mat', 'N15_M07_F10_KA15_8.mat', 'N15_M07_F10_KA15_9.mat', 'N15_M07_F10_KA15_19.mat', 'N15_M07_F10_KA15_18.mat']\n",
      "Y is:\n",
      "2\n",
      "filestring\n",
      "N15_M07_F10_\n",
      "l: \n",
      "[['KA07', '1500', '400', 'http://groups.uni-paderborn.de/kat/BearingDataCenter/KA07.rar']]\n",
      "file to exrtract is is::\n",
      "./data/PDB/KA07/raw/KA07.rar\n",
      "file already extracted, skipping unrar\n",
      "sorted filelist:\n",
      "['N15_M07_F10_KA07_15.mat', 'N15_M07_F10_KA07_14.mat', 'N15_M07_F10_KA07_16.mat', 'N15_M07_F10_KA07_17.mat', 'N15_M07_F10_KA07_13.mat', 'N15_M07_F10_KA07_12.mat', 'N15_M07_F10_KA07_10.mat', 'N15_M07_F10_KA07_11.mat', 'N15_M07_F10_KA07_8.mat', 'N15_M07_F10_KA07_9.mat', 'N15_M07_F10_KA07_7.mat', 'N15_M07_F10_KA07_6.mat', 'N15_M07_F10_KA07_4.mat', 'N15_M07_F10_KA07_5.mat', 'N15_M07_F10_KA07_1.mat', 'N15_M07_F10_KA07_2.mat', 'N15_M07_F10_KA07_3.mat', 'N15_M07_F10_KA07_20.mat', 'N15_M07_F10_KA07_19.mat', 'N15_M07_F10_KA07_18.mat']\n",
      "Y is:\n",
      "2\n",
      "filestring\n",
      "N15_M07_F10_\n",
      "l: \n",
      "[['KA30', '1500', '400', 'http://groups.uni-paderborn.de/kat/BearingDataCenter/KA30.rar']]\n",
      "file to exrtract is is::\n",
      "./data/PDB/KA30/raw/KA30.rar\n",
      "file already extracted, skipping unrar\n",
      "sorted filelist:\n",
      "['N15_M07_F10_KA30_5.mat', 'N15_M07_F10_KA30_4.mat', 'N15_M07_F10_KA30_6.mat', 'N15_M07_F10_KA30_20.mat', 'N15_M07_F10_KA30_7.mat', 'N15_M07_F10_KA30_3.mat', 'N15_M07_F10_KA30_19.mat', 'N15_M07_F10_KA30_18.mat', 'N15_M07_F10_KA30_2.mat', 'N15_M07_F10_KA30_1.mat', 'N15_M07_F10_KA30_16.mat', 'N15_M07_F10_KA30_17.mat', 'N15_M07_F10_KA30_15.mat', 'N15_M07_F10_KA30_14.mat', 'N15_M07_F10_KA30_10.mat', 'N15_M07_F10_KA30_11.mat', 'N15_M07_F10_KA30_9.mat', 'N15_M07_F10_KA30_13.mat', 'N15_M07_F10_KA30_12.mat', 'N15_M07_F10_KA30_8.mat']\n",
      "Y is:\n",
      "2\n",
      "filestring\n",
      "N15_M07_F10_\n",
      "l: \n",
      "[['KA16', '1500', '400', 'http://groups.uni-paderborn.de/kat/BearingDataCenter/KA16.rar']]\n",
      "file to exrtract is is::\n",
      "./data/PDB/KA16/raw/KA16.rar\n",
      "file already extracted, skipping unrar\n",
      "sorted filelist:\n",
      "['N15_M07_F10_KA16_6.mat', 'N15_M07_F10_KA16_7.mat', 'N15_M07_F10_KA16_18.mat', 'N15_M07_F10_KA16_5.mat', 'N15_M07_F10_KA16_4.mat', 'N15_M07_F10_KA16_19.mat', 'N15_M07_F10_KA16_1.mat', 'N15_M07_F10_KA16_20.mat', 'N15_M07_F10_KA16_3.mat', 'N15_M07_F10_KA16_2.mat', 'N15_M07_F10_KA16_12.mat', 'N15_M07_F10_KA16_13.mat', 'N15_M07_F10_KA16_11.mat', 'N15_M07_F10_KA16_10.mat', 'N15_M07_F10_KA16_14.mat', 'N15_M07_F10_KA16_9.mat', 'N15_M07_F10_KA16_8.mat', 'N15_M07_F10_KA16_15.mat', 'N15_M07_F10_KA16_17.mat', 'N15_M07_F10_KA16_16.mat']\n",
      "Y is:\n",
      "3\n",
      "filestring\n",
      "N15_M07_F10_\n",
      "l: \n",
      "[['KI14', '1500', '400', 'http://groups.uni-paderborn.de/kat/BearingDataCenter/KI14.rar']]\n",
      "file to exrtract is is::\n",
      "./data/PDB/KI14/raw/KI14.rar\n",
      "file already extracted, skipping unrar\n",
      "sorted filelist:\n",
      "['N15_M07_F10_KI14_11.mat', 'N15_M07_F10_KI14_10.mat', 'N15_M07_F10_KI14_12.mat', 'N15_M07_F10_KI14_13.mat', 'N15_M07_F10_KI14_17.mat', 'N15_M07_F10_KI14_16.mat', 'N15_M07_F10_KI14_14.mat', 'N15_M07_F10_KI14_15.mat', 'N15_M07_F10_KI14_2.mat', 'N15_M07_F10_KI14_3.mat', 'N15_M07_F10_KI14_1.mat', 'N15_M07_F10_KI14_4.mat', 'N15_M07_F10_KI14_5.mat', 'N15_M07_F10_KI14_7.mat', 'N15_M07_F10_KI14_6.mat', 'N15_M07_F10_KI14_8.mat', 'N15_M07_F10_KI14_9.mat', 'N15_M07_F10_KI14_18.mat', 'N15_M07_F10_KI14_19.mat', 'N15_M07_F10_KI14_20.mat']\n",
      "Y is:\n",
      "3\n",
      "filestring\n",
      "N15_M07_F10_\n",
      "l: \n",
      "[['KI01', '1500', '400', 'http://groups.uni-paderborn.de/kat/BearingDataCenter/KI01.rar']]\n",
      "file to exrtract is is::\n",
      "./data/PDB/KI01/raw/KI01.rar\n",
      "file already extracted, skipping unrar\n",
      "sorted filelist:\n",
      "['N15_M07_F10_KI01_18.mat', 'N15_M07_F10_KI01_19.mat', 'N15_M07_F10_KI01_20.mat', 'N15_M07_F10_KI01_8.mat', 'N15_M07_F10_KI01_9.mat', 'N15_M07_F10_KI01_7.mat', 'N15_M07_F10_KI01_6.mat', 'N15_M07_F10_KI01_4.mat', 'N15_M07_F10_KI01_5.mat', 'N15_M07_F10_KI01_1.mat', 'N15_M07_F10_KI01_2.mat', 'N15_M07_F10_KI01_3.mat', 'N15_M07_F10_KI01_11.mat', 'N15_M07_F10_KI01_10.mat', 'N15_M07_F10_KI01_12.mat', 'N15_M07_F10_KI01_13.mat', 'N15_M07_F10_KI01_17.mat', 'N15_M07_F10_KI01_16.mat', 'N15_M07_F10_KI01_14.mat', 'N15_M07_F10_KI01_15.mat']\n",
      "Y is:\n",
      "3\n",
      "filestring\n",
      "N15_M07_F10_\n",
      "l: \n",
      "[['KI21', '1500', '400', 'http://groups.uni-paderborn.de/kat/BearingDataCenter/KI21.rar']]\n",
      "file to exrtract is is::\n",
      "./data/PDB/KI21/raw/KI21.rar\n",
      "file already extracted, skipping unrar\n",
      "sorted filelist:\n",
      "['N15_M07_F10_KI21_8.mat', 'N15_M07_F10_KI21_20.mat', 'N15_M07_F10_KI21_9.mat', 'N15_M07_F10_KI21_18.mat', 'N15_M07_F10_KI21_19.mat', 'N15_M07_F10_KI21_2.mat', 'N15_M07_F10_KI21_17.mat', 'N15_M07_F10_KI21_16.mat', 'N15_M07_F10_KI21_3.mat', 'N15_M07_F10_KI21_1.mat', 'N15_M07_F10_KI21_14.mat', 'N15_M07_F10_KI21_15.mat', 'N15_M07_F10_KI21_4.mat', 'N15_M07_F10_KI21_11.mat', 'N15_M07_F10_KI21_10.mat', 'N15_M07_F10_KI21_5.mat', 'N15_M07_F10_KI21_7.mat', 'N15_M07_F10_KI21_12.mat', 'N15_M07_F10_KI21_13.mat', 'N15_M07_F10_KI21_6.mat']\n",
      "Y is:\n",
      "3\n",
      "filestring\n",
      "N15_M07_F10_\n",
      "l: \n",
      "[['KI05', '1500', '400', 'http://groups.uni-paderborn.de/kat/BearingDataCenter/KI05.rar']]\n",
      "file to exrtract is is::\n",
      "./data/PDB/KI05/raw/KI05.rar\n",
      "file already extracted, skipping unrar\n",
      "sorted filelist:\n",
      "['N15_M07_F10_KI05_20.mat', 'N15_M07_F10_KI05_9.mat', 'N15_M07_F10_KI05_8.mat', 'N15_M07_F10_KI05_19.mat', 'N15_M07_F10_KI05_18.mat', 'N15_M07_F10_KI05_16.mat', 'N15_M07_F10_KI05_3.mat', 'N15_M07_F10_KI05_2.mat', 'N15_M07_F10_KI05_17.mat', 'N15_M07_F10_KI05_15.mat', 'N15_M07_F10_KI05_1.mat', 'N15_M07_F10_KI05_14.mat', 'N15_M07_F10_KI05_10.mat', 'N15_M07_F10_KI05_5.mat', 'N15_M07_F10_KI05_4.mat', 'N15_M07_F10_KI05_11.mat', 'N15_M07_F10_KI05_13.mat', 'N15_M07_F10_KI05_6.mat', 'N15_M07_F10_KI05_7.mat', 'N15_M07_F10_KI05_12.mat']\n",
      "Y is:\n",
      "3\n",
      "filestring\n",
      "N15_M07_F10_\n",
      "l: \n",
      "[['KI17', '1500', '400', 'http://groups.uni-paderborn.de/kat/BearingDataCenter/KI17.rar']]\n",
      "file to exrtract is is::\n",
      "./data/PDB/KI17/raw/KI17.rar\n",
      "file already extracted, skipping unrar\n",
      "sorted filelist:\n",
      "['N15_M07_F10_KI17_3.mat', 'N15_M07_F10_KI17_2.mat', 'N15_M07_F10_KI17_1.mat', 'N15_M07_F10_KI17_5.mat', 'N15_M07_F10_KI17_4.mat', 'N15_M07_F10_KI17_6.mat', 'N15_M07_F10_KI17_7.mat', 'N15_M07_F10_KI17_20.mat', 'N15_M07_F10_KI17_19.mat', 'N15_M07_F10_KI17_18.mat', 'N15_M07_F10_KI17_16.mat', 'N15_M07_F10_KI17_17.mat', 'N15_M07_F10_KI17_15.mat', 'N15_M07_F10_KI17_14.mat', 'N15_M07_F10_KI17_10.mat', 'N15_M07_F10_KI17_11.mat', 'N15_M07_F10_KI17_13.mat', 'N15_M07_F10_KI17_12.mat', 'N15_M07_F10_KI17_9.mat', 'N15_M07_F10_KI17_8.mat']\n",
      "Y is:\n",
      "3\n",
      "filestring\n",
      "N15_M07_F10_\n",
      "l: \n",
      "[['KI07', '1500', '400', 'http://groups.uni-paderborn.de/kat/BearingDataCenter/KI07.rar']]\n",
      "file to exrtract is is::\n",
      "./data/PDB/KI07/raw/KI07.rar\n",
      "file already extracted, skipping unrar\n",
      "sorted filelist:\n",
      "['N15_M07_F10_KI07_8.mat', 'N15_M07_F10_KI07_9.mat', 'N15_M07_F10_KI07_19.mat', 'N15_M07_F10_KI07_18.mat', 'N15_M07_F10_KI07_20.mat', 'N15_M07_F10_KI07_1.mat', 'N15_M07_F10_KI07_13.mat', 'N15_M07_F10_KI07_12.mat', 'N15_M07_F10_KI07_2.mat', 'N15_M07_F10_KI07_10.mat', 'N15_M07_F10_KI07_11.mat', 'N15_M07_F10_KI07_3.mat', 'N15_M07_F10_KI07_7.mat', 'N15_M07_F10_KI07_15.mat', 'N15_M07_F10_KI07_14.mat', 'N15_M07_F10_KI07_6.mat', 'N15_M07_F10_KI07_4.mat', 'N15_M07_F10_KI07_16.mat', 'N15_M07_F10_KI07_17.mat', 'N15_M07_F10_KI07_5.mat']\n",
      "Y is:\n",
      "3\n",
      "filestring\n",
      "N15_M07_F10_\n",
      "l: \n",
      "[['KI18', '1500', '400', 'http://groups.uni-paderborn.de/kat/BearingDataCenter/KI18.rar']]\n",
      "file to exrtract is is::\n",
      "./data/PDB/KI18/raw/KI18.rar\n",
      "file already extracted, skipping unrar\n",
      "sorted filelist:\n",
      "['N15_M07_F10_KI18_11.mat', 'N15_M07_F10_KI18_10.mat', 'N15_M07_F10_KI18_12.mat', 'N15_M07_F10_KI18_13.mat', 'N15_M07_F10_KI18_17.mat', 'N15_M07_F10_KI18_16.mat', 'N15_M07_F10_KI18_14.mat', 'N15_M07_F10_KI18_15.mat', 'N15_M07_F10_KI18_9.mat', 'N15_M07_F10_KI18_8.mat', 'N15_M07_F10_KI18_6.mat', 'N15_M07_F10_KI18_7.mat', 'N15_M07_F10_KI18_5.mat', 'N15_M07_F10_KI18_4.mat', 'N15_M07_F10_KI18_1.mat', 'N15_M07_F10_KI18_3.mat', 'N15_M07_F10_KI18_2.mat', 'N15_M07_F10_KI18_18.mat', 'N15_M07_F10_KI18_19.mat', 'N15_M07_F10_KI18_20.mat']\n",
      "Y is:\n",
      "3\n",
      "filestring\n",
      "N15_M07_F10_\n",
      "l: \n",
      "[['KI16', '1500', '400', 'http://groups.uni-paderborn.de/kat/BearingDataCenter/KI16.rar']]\n",
      "file to exrtract is is::\n",
      "./data/PDB/KI16/raw/KI16.rar\n",
      "file already extracted, skipping unrar\n",
      "sorted filelist:\n",
      "['N15_M07_F10_KI16_14.mat', 'N15_M07_F10_KI16_15.mat', 'N15_M07_F10_KI16_17.mat', 'N15_M07_F10_KI16_16.mat', 'N15_M07_F10_KI16_12.mat', 'N15_M07_F10_KI16_13.mat', 'N15_M07_F10_KI16_11.mat', 'N15_M07_F10_KI16_10.mat', 'N15_M07_F10_KI16_1.mat', 'N15_M07_F10_KI16_3.mat', 'N15_M07_F10_KI16_2.mat', 'N15_M07_F10_KI16_6.mat', 'N15_M07_F10_KI16_7.mat', 'N15_M07_F10_KI16_5.mat', 'N15_M07_F10_KI16_4.mat', 'N15_M07_F10_KI16_9.mat', 'N15_M07_F10_KI16_8.mat', 'N15_M07_F10_KI16_20.mat', 'N15_M07_F10_KI16_18.mat', 'N15_M07_F10_KI16_19.mat']\n",
      "##################### \n",
      " datacollection and unpack done\n"
     ]
    }
   ],
   "source": [
    "#healthy sets\n",
    "d_healthy_train_1 = PDB('K002','1500','1000','700',SliceLen) #\n",
    "d_healthy_test_1  = PDB('K001','1500','1000','700',SliceLen)\n",
    "\n",
    "#outerring\n",
    "d_OR_test_1  = PDB('KA22','1500','1000','700',SliceLen) \n",
    "d_OR_train_2 = PDB('KA01','1500','1000','700',SliceLen) #EDM\n",
    "d_OR_test_2  = PDB('K004','1500','1000','700',SliceLen)\n",
    "d_OR_train_3 = PDB('KA05','1500','1000','700',SliceLen) #Elec engraved\n",
    "d_OR_test_3  = PDB('KA15','1500','1000','700',SliceLen)\n",
    "d_OR_train_4 = PDB('KA07','1500','1000','700',SliceLen) #Drilling\n",
    "d_OR_test_4  = PDB('KA30','1500','1000','700',SliceLen)\n",
    "d_OR_test_5  = PDB('KA16','1500','1000','700',SliceLen) \n",
    "\n",
    "#innerring\n",
    "d_IR_test_1  = PDB('KI14','1500','1000','700',SliceLen)\n",
    "d_IR_train_2 = PDB('KI01','1500','1000','700',SliceLen) #EDM\n",
    "d_IR_test_2  = PDB('KI21','1500','1000','700',SliceLen)\n",
    "d_IR_train_3 = PDB('KI05','1500','1000','700',SliceLen) #Elct engraved\n",
    "d_IR_test_3  = PDB('KI17','1500','1000','700',SliceLen)\n",
    "d_IR_train_4 = PDB('KI07','1500','1000','700',SliceLen) #Elec engraved\n",
    "d_IR_test_4  = PDB('KI18','1500','1000','700',SliceLen)\n",
    "d_IR_test_5  = PDB('KI16','1500','1000','700',SliceLen) \n",
    "\n",
    "print(\"##################### \\n datacollection and unpack done\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59609, 500)\n",
      "(59609,)\n"
     ]
    }
   ],
   "source": [
    "# These data are from Artificially made faults\n",
    "X_train = np.concatenate([d_healthy_train_1.X_train, \n",
    "                          d_OR_train_2.X_train,d_OR_train_3.X_train, \n",
    "                          d_OR_train_4.X_train,\n",
    "                          d_IR_train_2.X_train,d_IR_train_3.X_train,\n",
    "                          d_IR_train_4.X_train])\n",
    "\n",
    "y_train = np.concatenate([d_healthy_train_1.y_train, \n",
    "                          d_OR_train_2.y_train,d_OR_train_3.y_train, \n",
    "                          d_OR_train_4.y_train, \n",
    "                          d_IR_train_2.y_train,d_IR_train_3.y_train,\n",
    "                          d_IR_train_4.y_train])\n",
    "\n",
    "#traning data\n",
    "print(np.shape(X_train))\n",
    "print(np.shape(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60449, 500)\n",
      "(60449,)\n"
     ]
    }
   ],
   "source": [
    "#test data, NOTE from the PDB class, when using to generate data, not split by test/training, \n",
    "#all data is to be read from \"class.<X/y>.train\"\n",
    "X_test = np.concatenate([d_healthy_test_1.X_train, \n",
    "                          d_OR_test_2.X_train,d_OR_test_3.X_train, \n",
    "                          d_OR_test_4.X_train,\n",
    "                          d_IR_test_2.X_train,d_IR_test_3.X_train,\n",
    "                          d_IR_test_4.X_train])\n",
    "\n",
    "y_test = np.concatenate([d_healthy_test_1.y_train, \n",
    "                          d_OR_test_2.y_train,d_OR_test_3.y_train, \n",
    "                          d_OR_test_4.y_train, \n",
    "                          d_IR_test_2.y_train,d_IR_test_3.y_train,\n",
    "                          d_IR_test_4.y_train])\n",
    "#testing data\n",
    "print(np.shape(X_test))\n",
    "print(np.shape(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Compose traning & test datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How much data did we get ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19048829104 bytes\n"
     ]
    }
   ],
   "source": [
    "def get_size(start_path = '.'):\n",
    "    total_size = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(start_path):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            # skip if it is symbolic link\n",
    "            if not os.path.islink(fp):\n",
    "                total_size += os.path.getsize(fp)\n",
    "\n",
    "    return total_size\n",
    "\n",
    "print(get_size('./data'), 'bytes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#rcParams['figure.figsize'] = 11.7,8.27\n",
    "\n",
    "#show every 200 values\n",
    "plt.plot(y_train[0::200])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Is data biassed / unbalanced\n",
    "count the instances in each of the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 9260, 2: 22651, 3: 27698}\n"
     ]
    }
   ],
   "source": [
    "# any bias ? \n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Build a tensorflow model\n",
    "We build the WDCNN model introduced in this article [https://www.researchgate.net/publication/314247372_A_New_Deep_Learning_Model_for_Fault_Diagnosis_with_Good_Anti-Noise_and_Domain_Adaptation_Ability_on_Raw_Vibration_Signals](WDCNN)\n",
    "\n",
    "The proposed method uses raw vibration signals as input, and uses the wide kernels in the first convolutional layer for extracting features\n",
    "and suppressing high frequency noise.<br> Small convolutional kernels in the preceding layers are used\n",
    "for multilayer nonlinear mapping.<br> AdaBN is implemented to improve the domain adaptation ability\n",
    "of the model.\n",
    "\n",
    "<img src=\"doc/wdcnn1.png\" width=\"1000\" height=\"600\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "#from tensorflow import keras\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#from tensorflow import keras as k\n",
    "from keras import utils\n",
    "\n",
    "y_train_cat = utils.to_categorical(\n",
    "    y_train,\n",
    "    #num_classes=(1+max(d1.y_train)),\n",
    "    num_classes= 4,\n",
    "    dtype='float32'\n",
    ")\n",
    "\n",
    "y_test_cat = utils.to_categorical(\n",
    "    y_test,\n",
    "    #num_classes=(1+max(d1.y_train)),\n",
    "    num_classes= 4,\n",
    "    dtype='float32'\n",
    ")\n",
    "\n",
    "#X_train = d1.X_train\n",
    "#X_test = d1.X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59609, 500)\n",
      "(60449, 500)\n",
      "(59609, 4)\n",
      "(60449, 4)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(X_train))\n",
    "print(np.shape(X_test))\n",
    "print(np.shape(y_train_cat))\n",
    "print(np.shape(y_test_cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#y_test_cat[0::330]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, InputLayer, Dropout, Conv1D, Flatten, Reshape, MaxPooling1D, BatchNormalization, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "import os\n",
    "\n",
    "#use GPU acceleration\n",
    "os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\"\n",
    "\n",
    "\n",
    "# Defining the convolution layer\n",
    "def wdcnn(filters, kernerl_size, strides, conv_padding, pool_padding,  pool_size, BatchNormal):\n",
    "    \"\"\"wdcnn Layer neuron\n",
    "\n",
    "    :param filters: Number of convolution kernels, integer\n",
    "    :param kernerl_size: Convolution kernel size, integer\n",
    "    :param strides: Step size, integer\n",
    "    :param conv_padding: 'same','valid'\n",
    "    :param pool_padding: 'same','valid'\n",
    "    :param pool_size: Pooled layer core size, integer\n",
    "    :param BatchNormal: Whether Batchnormal, Boolean\n",
    "    :return: model\n",
    "    \"\"\"\n",
    "    model.add(Conv1D(filters=filters, kernel_size=kernerl_size, strides=strides,\n",
    "                     padding=conv_padding, kernel_regularizer=regularizers.l2(1e-4)))\n",
    "    if BatchNormal:\n",
    "        model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling1D(pool_size=pool_size, padding=pool_padding))\n",
    "    return model\n",
    "\n",
    "\n",
    "#### from wdcnnn\n",
    "BatchNorm = True # Whether to batch normalize\n",
    "classes = 4#(max(d1.y_train)+1)\n",
    "\n",
    "# Instantiated sequential model\n",
    "model = Sequential()\n",
    "model.add(InputLayer(input_shape=(X_train.shape[1], ), name='x_input'))\n",
    "model.add(Reshape((int(X_train.shape[1] / 1), 1), input_shape=(X_train.shape[1], )))\n",
    "# Set up the input layer, the first layer of convolution. Because you want to specify input_shape, it is released separately.#model.add(Conv1D(filters=16, kernel_size=64, strides=16, padding='same',kernel_regularizer=l2(1e-4), input_shape=input_shape))\n",
    "model.add(Conv1D(filters=16, kernel_size=64, strides=16, padding='same',kernel_regularizer=regularizers.l2(1e-4)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "#model.add(MaxPooling1D(pool_size=2))\n",
    "# second layer conv\n",
    "model = wdcnn(filters=32, kernerl_size=3, strides=1, conv_padding='same',pool_padding='valid',  pool_size=2, BatchNormal=BatchNorm)\n",
    "# Third layer conv\n",
    "model = wdcnn(filters=64, kernerl_size=3, strides=1, conv_padding='same',pool_padding='valid', pool_size=2, BatchNormal=BatchNorm)\n",
    "# Fourth layer conv\n",
    "model = wdcnn(filters=64, kernerl_size=3, strides=1, conv_padding='same',pool_padding='valid', pool_size=2, BatchNormal=BatchNorm)\n",
    "# Fifth layer conv\n",
    "model = wdcnn(filters=64, kernerl_size=3, strides=1, conv_padding='valid',pool_padding='valid', pool_size=2, BatchNormal=BatchNorm)\n",
    "# Flatten from convolution to full connection\n",
    "model.add(Flatten())\n",
    "\n",
    "# Flatten from convolution to full connection\n",
    "model.add(Dense(units=100, activation='relu', kernel_regularizer=regularizers.l2(1e-4)))\n",
    "# Decrease the output layer\n",
    "#model.add(Dense(units=num_classes, activation='softmax', kernel_regularizer=regularizers.l2(1e-4)))\n",
    "model.add(Dense(classes, activation='softmax', name='y_pred'))\n",
    "\n",
    "\n",
    "# this controls the learning rate\n",
    "opt = Adam(lr=0.0005, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape (Reshape)            (None, 500, 1)            0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 32, 16)            1040      \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 32, 16)            64        \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 32, 16)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 32, 32)            1568      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32)            128       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 16, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 16, 64)            6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 16, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 8, 64)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 8, 64)             12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 8, 64)             256       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 8, 64)             0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 4, 64)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 2, 64)             12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 2, 64)             256       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 2, 64)             0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 1, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               6500      \n",
      "_________________________________________________________________\n",
      "y_pred (Dense)               (None, 4)                 404       \n",
      "=================================================================\n",
      "Total params: 41,384\n",
      "Trainable params: 40,904\n",
      "Non-trainable params: 480\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam', loss='categorical_crossentropy',metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39938, 500)\n",
      "(19671, 500)\n",
      "(39938,)\n",
      "(19671,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.33, random_state=42)\n",
    "\n",
    "print(np.shape(X_train))\n",
    "print(np.shape(X_test))\n",
    "print(np.shape(y_train))\n",
    "print(np.shape(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from keras import utils\n",
    "\n",
    "y_train_cat = utils.to_categorical(\n",
    "    y_train,\n",
    "    #num_classes=(1+max(d1.y_train)),\n",
    "    num_classes= 4,\n",
    "    dtype='float32'\n",
    ")\n",
    "\n",
    "y_test_cat = utils.to_categorical(\n",
    "    y_test,\n",
    "    #num_classes=(1+max(d1.y_train)),\n",
    "    num_classes= 4,\n",
    "    dtype='float32'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "799/799 [==============================] - 10s 12ms/step - loss: 0.4939 - accuracy: 0.7770 - val_loss: 0.3718 - val_accuracy: 0.8352\n",
      "Epoch 2/100\n",
      "799/799 [==============================] - 9s 11ms/step - loss: 0.3539 - accuracy: 0.8456 - val_loss: 0.3509 - val_accuracy: 0.8468\n",
      "Epoch 3/100\n",
      "799/799 [==============================] - 8s 10ms/step - loss: 0.3174 - accuracy: 0.8655 - val_loss: 0.3582 - val_accuracy: 0.8378\n",
      "Epoch 4/100\n",
      "799/799 [==============================] - 8s 10ms/step - loss: 0.2923 - accuracy: 0.8773 - val_loss: 0.3034 - val_accuracy: 0.8722\n",
      "Epoch 5/100\n",
      "799/799 [==============================] - 9s 11ms/step - loss: 0.2742 - accuracy: 0.8886 - val_loss: 0.3162 - val_accuracy: 0.8715\n",
      "Epoch 6/100\n",
      "799/799 [==============================] - 9s 11ms/step - loss: 0.2605 - accuracy: 0.8964 - val_loss: 0.3271 - val_accuracy: 0.8661\n",
      "Epoch 7/100\n",
      "799/799 [==============================] - 8s 10ms/step - loss: 0.2465 - accuracy: 0.9039 - val_loss: 0.2755 - val_accuracy: 0.8871\n",
      "Epoch 8/100\n",
      "799/799 [==============================] - 9s 11ms/step - loss: 0.2355 - accuracy: 0.9104 - val_loss: 0.4102 - val_accuracy: 0.8429\n",
      "Epoch 9/100\n",
      "799/799 [==============================] - 7s 9ms/step - loss: 0.2246 - accuracy: 0.9158 - val_loss: 0.3721 - val_accuracy: 0.8516\n",
      "Epoch 10/100\n",
      "799/799 [==============================] - 7s 9ms/step - loss: 0.2153 - accuracy: 0.9206 - val_loss: 0.4016 - val_accuracy: 0.8400\n",
      "Epoch 11/100\n",
      "799/799 [==============================] - 7s 9ms/step - loss: 0.2084 - accuracy: 0.9243 - val_loss: 0.3403 - val_accuracy: 0.8689\n",
      "Epoch 12/100\n",
      "799/799 [==============================] - 8s 11ms/step - loss: 0.1954 - accuracy: 0.9317 - val_loss: 0.3562 - val_accuracy: 0.8668\n",
      "Epoch 13/100\n",
      "799/799 [==============================] - 8s 10ms/step - loss: 0.1922 - accuracy: 0.9332 - val_loss: 0.4025 - val_accuracy: 0.8537\n",
      "Epoch 14/100\n",
      "799/799 [==============================] - 7s 9ms/step - loss: 0.1836 - accuracy: 0.9366 - val_loss: 0.3187 - val_accuracy: 0.8812\n",
      "Epoch 15/100\n",
      "799/799 [==============================] - 8s 10ms/step - loss: 0.1778 - accuracy: 0.9421 - val_loss: 0.3285 - val_accuracy: 0.8795\n",
      "Epoch 16/100\n",
      "799/799 [==============================] - 8s 10ms/step - loss: 0.1736 - accuracy: 0.9432 - val_loss: 0.3198 - val_accuracy: 0.8834\n",
      "Epoch 17/100\n",
      "799/799 [==============================] - 9s 11ms/step - loss: 0.1681 - accuracy: 0.9457 - val_loss: 0.3923 - val_accuracy: 0.8654\n",
      "Epoch 18/100\n",
      "799/799 [==============================] - 9s 11ms/step - loss: 0.1608 - accuracy: 0.9503 - val_loss: 0.3684 - val_accuracy: 0.8735\n",
      "Epoch 19/100\n",
      "799/799 [==============================] - 7s 9ms/step - loss: 0.1561 - accuracy: 0.9524 - val_loss: 0.4758 - val_accuracy: 0.8533\n",
      "Epoch 20/100\n",
      "799/799 [==============================] - 9s 12ms/step - loss: 0.1509 - accuracy: 0.9549 - val_loss: 0.3615 - val_accuracy: 0.8860\n",
      "Epoch 21/100\n",
      "799/799 [==============================] - 8s 11ms/step - loss: 0.1484 - accuracy: 0.9557 - val_loss: 0.4281 - val_accuracy: 0.8686\n",
      "Epoch 22/100\n",
      "799/799 [==============================] - 8s 10ms/step - loss: 0.1466 - accuracy: 0.9576 - val_loss: 0.4011 - val_accuracy: 0.8739\n",
      "Epoch 23/100\n",
      "799/799 [==============================] - 9s 11ms/step - loss: 0.1396 - accuracy: 0.9609 - val_loss: 0.3657 - val_accuracy: 0.8852\n",
      "Epoch 24/100\n",
      "799/799 [==============================] - 9s 11ms/step - loss: 0.1372 - accuracy: 0.9628 - val_loss: 0.4134 - val_accuracy: 0.8793\n",
      "Epoch 25/100\n",
      "359/799 [============>.................] - ETA: 4s - loss: 0.1277 - accuracy: 0.9669"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-16bd027a3082>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_cat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train_cat, batch_size=50, epochs=100, validation_data=(X_test, y_test_cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluate model 1, artificial fault data\n",
    "predict, asess, and create a consfusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#X_test\n",
    "# Evaluate the model on the test data using `evaluate`\n",
    "print('\\n# Evaluate on test data')\n",
    "results = model.evaluate(X_test, y_test_cat, batch_size=128)\n",
    "#results = model.evaluate(X_test, Y_test, batch_size=50)\n",
    "print('test loss, test acc:', results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "model.save('model_1_inputw500.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.plot()#tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# import for showing the confusion matrix\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "results = model.predict(X_test, 128)# batch_size=HParams.test_batch_size)\n",
    "\n",
    "# convert from class probabilities to actual class predictions\n",
    "predicted_classes = np.argmax(results, axis=1)\n",
    "\n",
    "# Names of predicted classes\n",
    "class_names = [ \"OK\", \"Outer\", \"Inner\", \"Combined\"]\n",
    "#Labels 1 = healthy bearing, 2 = outer ring damage , 3 = inner ring damage, 4 = combined damage\n",
    "\n",
    "\n",
    "# Generate the confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, predicted_classes)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Conclusion on the results\n",
    "The above model was trained with a small inpout dataset-slice-width of 500 samples, and accordingly shape of the first input layer <br>\n",
    "This corresponds to roughly 1/5 of a full resolution when sampling at 64KHz, and therefore inne ring damages are not seen as distinct, since the damage will not roll through the load zone more than every 5'th time.<br>\n",
    "The close to 50% accuracy may suggest **some overfitting of the model** ?\n",
    "\n",
    "This model was build with :\n",
    "* Total params: 41,384\n",
    "* Trainable params: 40,904\n",
    "* Non-trainable params: 480\n",
    "\n",
    "And a result of :\n",
    "    **test loss, test acc: [0.5964195777071092, 0.8777896]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluate model, setting 2, artificial fault data\n",
    "Now redo with a full revolution of samples, as input shape<br>\n",
    "The first exepriment used 500 as input dimension, now we ramp it up to 2560 (samples from a full revolution at 150RPM, Fs 64KHz), resulting in a larger model with\n",
    "* Total params: 92,584\n",
    "* Trainable params: 92,104\n",
    "* Non-trainable params: 480\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "SliceLen = SamplePerRotation\n",
    "#healthy sets\n",
    "d_healthy_train_1 = PDB('K002','1500','1000','700',SliceLen)\n",
    "d_healthy_test_1  = PDB('K001','1500','1000','700',SliceLen)\n",
    "\n",
    "#outerring\n",
    "d_OR_test_1  = PDB('KA22','1500','1000','700',SliceLen)\n",
    "d_OR_train_2 = PDB('KA01','1500','1000','700',SliceLen)\n",
    "d_OR_test_2  = PDB('K004','1500','1000','700',SliceLen)\n",
    "d_OR_train_3 = PDB('KA05','1500','1000','700',SliceLen)\n",
    "d_OR_test_3  = PDB('KA15','1500','1000','700',SliceLen)\n",
    "d_OR_train_4 = PDB('KA07','1500','1000','700',SliceLen)\n",
    "d_OR_test_4  = PDB('KA30','1500','1000','700',SliceLen)\n",
    "d_OR_test_5  = PDB('KA16','1500','1000','700',SliceLen)\n",
    "\n",
    "#innerring\n",
    "d_IR_test_1  = PDB('KI14','1500','1000','700',SliceLen)\n",
    "d_IR_train_2 = PDB('KI01','1500','1000','700',SliceLen)\n",
    "d_IR_test_2  = PDB('KI21','1500','1000','700',SliceLen)\n",
    "d_IR_train_3 = PDB('KI05','1500','1000','700',SliceLen)\n",
    "d_IR_test_3  = PDB('KI17','1500','1000','700',SliceLen)\n",
    "d_IR_train_4 = PDB('KI07','1500','1000','700',SliceLen)\n",
    "d_IR_test_4  = PDB('KI18','1500','1000','700',SliceLen)\n",
    "d_IR_test_5  = PDB('KI16','1500','1000','700',SliceLen)\n",
    "\n",
    "\n",
    "print(\"##################### \\n datacollection and unpack done\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X_train = np.concatenate([d_healthy_train_1.X_train, \n",
    "                          d_OR_train_2.X_train,d_OR_train_3.X_train, \n",
    "                          d_OR_train_4.X_train,\n",
    "                          d_IR_train_2.X_train,d_IR_train_3.X_train,\n",
    "                          d_IR_train_4.X_train])\n",
    "\n",
    "y_train = np.concatenate([d_healthy_train_1.y_train, \n",
    "                          d_OR_train_2.y_train,d_OR_train_3.y_train, \n",
    "                          d_OR_train_4.y_train, \n",
    "                          d_IR_train_2.y_train,d_IR_train_3.y_train,\n",
    "                          d_IR_train_4.y_train])\n",
    "\n",
    "#traning data\n",
    "print(np.shape(X_train))\n",
    "print(np.shape(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# any bias ? \n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, InputLayer, Dropout, Conv1D, Flatten, Reshape, MaxPooling1D, BatchNormalization, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "import os\n",
    "\n",
    "#use GPU acceleration\n",
    "os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\"\n",
    "\n",
    "\n",
    "# Defining the convolution layer\n",
    "def wdcnn(filters, kernerl_size, strides, conv_padding, pool_padding,  pool_size, BatchNormal):\n",
    "    \"\"\"wdcnn Layer neuron\n",
    "\n",
    "    :param filters: Number of convolution kernels, integer\n",
    "    :param kernerl_size: Convolution kernel size, integer\n",
    "    :param strides: Step size, integer\n",
    "    :param conv_padding: 'same','valid'\n",
    "    :param pool_padding: 'same','valid'\n",
    "    :param pool_size: Pooled layer core size, integer\n",
    "    :param BatchNormal: Whether Batchnormal, Boolean\n",
    "    :return: model\n",
    "    \"\"\"\n",
    "    model.add(Conv1D(filters=filters, kernel_size=kernerl_size, strides=strides,\n",
    "                     padding=conv_padding, kernel_regularizer=regularizers.l2(1e-4)))\n",
    "    if BatchNormal:\n",
    "        model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling1D(pool_size=pool_size, padding=pool_padding))\n",
    "    return model\n",
    "\n",
    "\n",
    "#### from wdcnnn\n",
    "BatchNorm = True # Whether to batch normalize\n",
    "classes = 4#(max(d1.y_train)+1)\n",
    "\n",
    "# Instantiated sequential model\n",
    "model = Sequential()\n",
    "model.add(InputLayer(input_shape=(X_train.shape[1], ), name='x_input'))\n",
    "model.add(Reshape((int(X_train.shape[1] / 1), 1), input_shape=(X_train.shape[1], )))\n",
    "# Set up the input layer, the first layer of convolution. Because you want to specify input_shape, it is released separately.#model.add(Conv1D(filters=16, kernel_size=64, strides=16, padding='same',kernel_regularizer=l2(1e-4), input_shape=input_shape))\n",
    "model.add(Conv1D(filters=16, kernel_size=64, strides=16, padding='same',kernel_regularizer=regularizers.l2(1e-4)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "#model.add(MaxPooling1D(pool_size=2))\n",
    "# second layer conv\n",
    "model = wdcnn(filters=32, kernerl_size=3, strides=1, conv_padding='same',pool_padding='valid',  pool_size=2, BatchNormal=BatchNorm)\n",
    "# Third layer conv\n",
    "model = wdcnn(filters=64, kernerl_size=3, strides=1, conv_padding='same',pool_padding='valid', pool_size=2, BatchNormal=BatchNorm)\n",
    "# Fourth layer conv\n",
    "model = wdcnn(filters=64, kernerl_size=3, strides=1, conv_padding='same',pool_padding='valid', pool_size=2, BatchNormal=BatchNorm)\n",
    "# Fifth layer conv\n",
    "model = wdcnn(filters=64, kernerl_size=3, strides=1, conv_padding='valid',pool_padding='valid', pool_size=2, BatchNormal=BatchNorm)\n",
    "# Flatten from convolution to full connection\n",
    "model.add(Flatten())\n",
    "\n",
    "# Flatten from convolution to full connection\n",
    "model.add(Dense(units=100, activation='relu', kernel_regularizer=regularizers.l2(1e-4)))\n",
    "# Decrease the output layer\n",
    "#model.add(Dense(units=num_classes, activation='softmax', kernel_regularizer=regularizers.l2(1e-4)))\n",
    "model.add(Dense(classes, activation='softmax', name='y_pred'))\n",
    "\n",
    "\n",
    "# this controls the learning rate\n",
    "opt = Adam(lr=0.0005, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras import utils\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.33, random_state=42)\n",
    "\n",
    "print(np.shape(X_train))\n",
    "print(np.shape(X_test))\n",
    "print(np.shape(y_train))\n",
    "print(np.shape(y_test))\n",
    "\n",
    "\n",
    "\n",
    "y_train_cat = utils.to_categorical(\n",
    "    y_train,\n",
    "    #num_classes=(1+max(d1.y_train)),\n",
    "    num_classes= 4,\n",
    "    dtype='float32'\n",
    ")\n",
    "\n",
    "y_test_cat = utils.to_categorical(\n",
    "    y_test,\n",
    "    #num_classes=(1+max(d1.y_train)),\n",
    "    num_classes= 4,\n",
    "    dtype='float32'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train_cat, batch_size=50, epochs=100, validation_data=(X_test, y_test_cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#X_test\n",
    "# Evaluate the model on the test data using `evaluate`\n",
    "print('\\n# Evaluate on test data')\n",
    "results = model.evaluate(X_test, y_test_cat, batch_size=128)\n",
    "#results = model.evaluate(X_test, Y_test, batch_size=50)\n",
    "print('test loss, test acc:', results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "686us/sample - loss: 0.0629 - accuracy: 0.9812\n",
    "test loss, test acc: [0.09110386756559213, 0.98125]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# import for showing the confusion matrix\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "results = model.predict(X_test, 128)# batch_size=HParams.test_batch_size)\n",
    "\n",
    "# convert from class probabilities to actual class predictions\n",
    "predicted_classes = np.argmax(results, axis=1)\n",
    "\n",
    "# Names of predicted classes\n",
    "class_names = [ \"OK\", \"Outer\", \"Inner\", \"Combined\"]\n",
    "#Labels 1 = healthy bearing, 2 = outer ring damage , 3 = inner ring damage, 4 = combined damage\n",
    "\n",
    "\n",
    "# Generate the confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, predicted_classes)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Conclusion on the results\n",
    "The above model was trained with a larger size first input layer of 2560, and accordingly shape of the first input layer <br>\n",
    "This corresponds to exactly 1/1 of a full resolution when sampling at 64KHz, and therefore inne ring damages are seen more as distinct, since the damage will not roll through the load zone every time.<br>\n",
    "The minor error  may suggest ???\n",
    "\n",
    "This model was build with :\n",
    "* Total params: 92,584\n",
    "* Trainable params: 92,104\n",
    "* Non-trainable params: 480\n",
    "\n",
    "And a result of :\n",
    "    **686us/sample - loss: 0.0629 - accuracy: 0.9812\n",
    "    test loss, test acc: [0.09110386756559213, 0.98125]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Use model.load(), to avoid retraining "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "model.save('model_2_inputw2560.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluate model, setting 2, actual fault data\n",
    "Now redo with a full revolution of samples, as input shape<br>\n",
    "Instead og using artifically generated data, data from the **HALT tests** are used\n",
    "* Total params: 92,584\n",
    "* Trainable params: 92,104\n",
    "* Non-trainable params: 480\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#test data, NOTE from the PDB class, when using to generate data, not split by test/training, \n",
    "#all data is to be read from \"class.<X/y>.train\"\n",
    "X_train_actual = np.concatenate([d_healthy_test_1.X_train, \n",
    "                          d_OR_test_2.X_train,d_OR_test_3.X_train, \n",
    "                          d_OR_test_4.X_train,\n",
    "                          d_IR_test_2.X_train,d_IR_test_3.X_train,\n",
    "                          d_IR_test_4.X_train])\n",
    "\n",
    "y_train_actual = np.concatenate([d_healthy_test_1.y_train, \n",
    "                          d_OR_test_2.y_train,d_OR_test_3.y_train, \n",
    "                          d_OR_test_4.y_train, \n",
    "                          d_IR_test_2.y_train,d_IR_test_3.y_train,\n",
    "                          d_IR_test_4.y_train])\n",
    "#testing data\n",
    "print(np.shape(X_train_actual))\n",
    "print(np.shape(y_train_actual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# any bias ? \n",
    "unique, counts = np.unique(y_train_actual, return_counts=True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, InputLayer, Dropout, Conv1D, Flatten, Reshape, MaxPooling1D, BatchNormalization, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "import os\n",
    "\n",
    "#use GPU acceleration\n",
    "os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\"\n",
    "\n",
    "\n",
    "# Defining the convolution layer\n",
    "def wdcnn(filters, kernerl_size, strides, conv_padding, pool_padding,  pool_size, BatchNormal):\n",
    "    \"\"\"wdcnn Layer neuron\n",
    "\n",
    "    :param filters: Number of convolution kernels, integer\n",
    "    :param kernerl_size: Convolution kernel size, integer\n",
    "    :param strides: Step size, integer\n",
    "    :param conv_padding: 'same','valid'\n",
    "    :param pool_padding: 'same','valid'\n",
    "    :param pool_size: Pooled layer core size, integer\n",
    "    :param BatchNormal: Whether Batchnormal, Boolean\n",
    "    :return: model\n",
    "    \"\"\"\n",
    "    model.add(Conv1D(filters=filters, kernel_size=kernerl_size, strides=strides,\n",
    "                     padding=conv_padding, kernel_regularizer=regularizers.l2(1e-4)))\n",
    "    if BatchNormal:\n",
    "        model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling1D(pool_size=pool_size, padding=pool_padding))\n",
    "    return model\n",
    "\n",
    "\n",
    "#### from wdcnnn\n",
    "BatchNorm = True # Whether to batch normalize\n",
    "classes = 4#(max(d1.y_train)+1)\n",
    "\n",
    "# Instantiated sequential model\n",
    "model = Sequential()\n",
    "model.add(InputLayer(input_shape=(X_train.shape[1], ), name='x_input'))\n",
    "model.add(Reshape((int(X_train.shape[1] / 1), 1), input_shape=(X_train.shape[1], )))\n",
    "# Set up the input layer, the first layer of convolution. Because you want to specify input_shape, it is released separately.#model.add(Conv1D(filters=16, kernel_size=64, strides=16, padding='same',kernel_regularizer=l2(1e-4), input_shape=input_shape))\n",
    "model.add(Conv1D(filters=16, kernel_size=64, strides=16, padding='same',kernel_regularizer=regularizers.l2(1e-4)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "#model.add(MaxPooling1D(pool_size=2))\n",
    "# second layer conv\n",
    "model = wdcnn(filters=32, kernerl_size=3, strides=1, conv_padding='same',pool_padding='valid',  pool_size=2, BatchNormal=BatchNorm)\n",
    "# Third layer conv\n",
    "model = wdcnn(filters=64, kernerl_size=3, strides=1, conv_padding='same',pool_padding='valid', pool_size=2, BatchNormal=BatchNorm)\n",
    "# Fourth layer conv\n",
    "model = wdcnn(filters=64, kernerl_size=3, strides=1, conv_padding='same',pool_padding='valid', pool_size=2, BatchNormal=BatchNorm)\n",
    "# Fifth layer conv\n",
    "model = wdcnn(filters=64, kernerl_size=3, strides=1, conv_padding='valid',pool_padding='valid', pool_size=2, BatchNormal=BatchNorm)\n",
    "# Flatten from convolution to full connection\n",
    "model.add(Flatten())\n",
    "\n",
    "# Flatten from convolution to full connection\n",
    "model.add(Dense(units=100, activation='relu', kernel_regularizer=regularizers.l2(1e-4)))\n",
    "# Decrease the output layer\n",
    "#model.add(Dense(units=num_classes, activation='softmax', kernel_regularizer=regularizers.l2(1e-4)))\n",
    "model.add(Dense(classes, activation='softmax', name='y_pred'))\n",
    "\n",
    "\n",
    "# this controls the learning rate\n",
    "opt = Adam(lr=0.0005, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras import utils\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_actual, y_train_actual, test_size=0.33, random_state=42)\n",
    "\n",
    "print(np.shape(X_train))\n",
    "print(np.shape(X_test))\n",
    "print(np.shape(y_train))\n",
    "print(np.shape(y_test))\n",
    "\n",
    "\n",
    "\n",
    "y_train_cat = utils.to_categorical(\n",
    "    y_train,\n",
    "    #num_classes=(1+max(d1.y_train)),\n",
    "    num_classes= 4,\n",
    "    dtype='float32'\n",
    ")\n",
    "\n",
    "y_test_cat = utils.to_categorical(\n",
    "    y_test,\n",
    "    #num_classes=(1+max(d1.y_train)),\n",
    "    num_classes= 4,\n",
    "    dtype='float32'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train_cat, batch_size=50, epochs=100, validation_data=(X_test, y_test_cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#X_test\n",
    "# Evaluate the model on the test data using `evaluate`\n",
    "print('\\n# Evaluate on test data')\n",
    "results = model.evaluate(X_test, y_test_cat, batch_size=128)\n",
    "#results = model.evaluate(X_test, Y_test, batch_size=50)\n",
    "print('test loss, test acc:', results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " 2s 565us/sample - loss: 0.0092 - accuracy: 1.0000\n",
    "test loss, test acc: [0.009345386033358868, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# import for showing the confusion matrix\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "results = model.predict(X_test, 128)# batch_size=HParams.test_batch_size)\n",
    "\n",
    "# convert from class probabilities to actual class predictions\n",
    "predicted_classes = np.argmax(results, axis=1)\n",
    "\n",
    "# Names of predicted classes\n",
    "class_names = [ \"OK\", \"Outer\", \"Inner\", \"Combined\"]\n",
    "#Labels 1 = healthy bearing, 2 = outer ring damage , 3 = inner ring damage, 4 = combined damage\n",
    "\n",
    "\n",
    "# Generate the confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, predicted_classes)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "model.save('model_2_inputw2560_actual_faultdata_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ls -al"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Conclusion on the results\n",
    "The above model was trained with a larger size first input layer of 2560, and accordingly shape of the first input layer <br>\n",
    "Data fro mthe **HALT tests** were used in this test<br>\n",
    "\n",
    "\n",
    "And a result of :\n",
    "    **]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio playback of data\n",
    "To try retraniung the model fro played back data, captured from a microphone, following is tested\n",
    "\n",
    "Data is sampled at 64 kHz\n",
    "Four different operating conditions (see operating conditions).\n",
    "20 measurements of 4 seconds each for each setting, saved as a MatLab file with a name consisting of the code of the operating condition and the four-digit bearing code (e.g. N15_M07_F10_KA01_1.mat).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bearings with outer ring damage\n",
    "|Bearing Code | Radial Load [N] | Speed ​​[min ^ -1] | Damage method | Damage extent (level) | Damage type |\n",
    "|--- | --- | --- | --- | --- | --- |\n",
    "| KA15 | - |-|HALT|1|Plastic deform.:Indentations|\n",
    "| KA16 | - |-|HALT|2|fatigue: pitting|\n",
    "| KA22 | - |-|HALT|1|fatigue: pitting|\n",
    "| KA30 | - |-|HALT|1|Plastic deform.:Indentations|\n",
    "\n",
    "### Bearings with inner ring damage\n",
    "|Bearing Code | Radial Load [N] | Speed ​​[min ^ -1] | Damage method | Damage extent (level) | Damage type |\n",
    "|--- | --- | --- | --- | --- | --- |\n",
    "| KI01 | - | - |  EDM |1|\n",
    "| KI04 | - | - | HALT |1|Fatigue,pitting|\n",
    "| KI14 | - | - | HALT |1|Fatigue,pitting |\n",
    "| KI16 | - | - | HALT |3|Fatigue,pitting |\n",
    "| KI17 | - | - | HALT |1|Fatigue,pitting |\n",
    "| KI18 | - | - | HALT |2|Fatigue,pitting |\n",
    "| KI21 | - | - | HALT |1|Fatigue,pitting |\n",
    "\n",
    "### Bearings with combined inner and outer ring damage\n",
    "|Bearing Code | Radial Load [N] | Speed ​​[min ^ -1] | Damage method | Damage extent (level) |Damage type |\n",
    "|--- | --- | --- | --- | --- | --- |\n",
    "| KB23 | - | - | HALT |2|fatigue: pitting|\n",
    "| KB24 | - | - | HALT |3|fatigue: pitting|\n",
    "| KB27 | - | - | HALT |1|Plastic deform.:Indentations|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y is:\n",
      "1\n",
      "filestring\n",
      "N15_M07_F10_\n",
      "l: \n",
      "[['K002', '1500', '400', 'http://groups.uni-paderborn.de/kat/BearingDataCenter/K002.rar']]\n",
      "file to exrtract is is::\n",
      "./data/PDB/K002/raw/K002.rar\n",
      "file already extracted, skipping unrar\n",
      "sorted filelist:\n",
      "['N15_M07_F10_K002_12.mat', 'N15_M07_F10_K002_13.mat', 'N15_M07_F10_K002_11.mat', 'N15_M07_F10_K002_10.mat', 'N15_M07_F10_K002_14.mat', 'N15_M07_F10_K002_15.mat', 'N15_M07_F10_K002_17.mat', 'N15_M07_F10_K002_8.mat', 'N15_M07_F10_K002_9.mat', 'N15_M07_F10_K002_16.mat', 'N15_M07_F10_K002_4.mat', 'N15_M07_F10_K002_5.mat', 'N15_M07_F10_K002_18.mat', 'N15_M07_F10_K002_7.mat', 'N15_M07_F10_K002_6.mat', 'N15_M07_F10_K002_19.mat', 'N15_M07_F10_K002_2.mat', 'N15_M07_F10_K002_3.mat', 'N15_M07_F10_K002_20.mat', 'N15_M07_F10_K002_1.mat']\n",
      "Y is:\n",
      "1\n",
      "filestring\n",
      "N15_M07_F10_\n",
      "l: \n",
      "[['K001', '1500', '400', 'http://groups.uni-paderborn.de/kat/BearingDataCenter/K001.rar']]\n",
      "file to exrtract is is::\n",
      "./data/PDB/K001/raw/K001.rar\n",
      "file already extracted, skipping unrar\n",
      "sorted filelist:\n",
      "['N15_M07_F10_K001_20.mat', 'N15_M07_F10_K001_19.mat', 'N15_M07_F10_K001_9.mat', 'N15_M07_F10_K001_8.mat', 'N15_M07_F10_K001_18.mat', 'N15_M07_F10_K001_15.mat', 'N15_M07_F10_K001_5.mat', 'N15_M07_F10_K001_4.mat', 'N15_M07_F10_K001_14.mat', 'N15_M07_F10_K001_16.mat', 'N15_M07_F10_K001_6.mat', 'N15_M07_F10_K001_7.mat', 'N15_M07_F10_K001_17.mat', 'N15_M07_F10_K001_3.mat', 'N15_M07_F10_K001_13.mat', 'N15_M07_F10_K001_12.mat', 'N15_M07_F10_K001_2.mat', 'N15_M07_F10_K001_10.mat', 'N15_M07_F10_K001_11.mat', 'N15_M07_F10_K001_1.mat']\n",
      "Y is:\n",
      "3\n",
      "filestring\n",
      "N15_M07_F10_\n",
      "l: \n",
      "[['KI17', '1500', '400', 'http://groups.uni-paderborn.de/kat/BearingDataCenter/KI17.rar']]\n",
      "file to exrtract is is::\n",
      "./data/PDB/KI17/raw/KI17.rar\n",
      "file already extracted, skipping unrar\n",
      "sorted filelist:\n",
      "['N15_M07_F10_KI17_3.mat', 'N15_M07_F10_KI17_2.mat', 'N15_M07_F10_KI17_1.mat', 'N15_M07_F10_KI17_5.mat', 'N15_M07_F10_KI17_4.mat', 'N15_M07_F10_KI17_6.mat', 'N15_M07_F10_KI17_7.mat', 'N15_M07_F10_KI17_20.mat', 'N15_M07_F10_KI17_19.mat', 'N15_M07_F10_KI17_18.mat', 'N15_M07_F10_KI17_16.mat', 'N15_M07_F10_KI17_17.mat', 'N15_M07_F10_KI17_15.mat', 'N15_M07_F10_KI17_14.mat', 'N15_M07_F10_KI17_10.mat', 'N15_M07_F10_KI17_11.mat', 'N15_M07_F10_KI17_13.mat', 'N15_M07_F10_KI17_12.mat', 'N15_M07_F10_KI17_9.mat', 'N15_M07_F10_KI17_8.mat']\n",
      "Y is:\n",
      "3\n",
      "filestring\n",
      "N15_M07_F10_\n",
      "l: \n",
      "[['KI16', '1500', '400', 'http://groups.uni-paderborn.de/kat/BearingDataCenter/KI16.rar']]\n",
      "file to exrtract is is::\n",
      "./data/PDB/KI16/raw/KI16.rar\n",
      "file already extracted, skipping unrar\n",
      "sorted filelist:\n",
      "['N15_M07_F10_KI16_14.mat', 'N15_M07_F10_KI16_15.mat', 'N15_M07_F10_KI16_17.mat', 'N15_M07_F10_KI16_16.mat', 'N15_M07_F10_KI16_12.mat', 'N15_M07_F10_KI16_13.mat', 'N15_M07_F10_KI16_11.mat', 'N15_M07_F10_KI16_10.mat', 'N15_M07_F10_KI16_1.mat', 'N15_M07_F10_KI16_3.mat', 'N15_M07_F10_KI16_2.mat', 'N15_M07_F10_KI16_6.mat', 'N15_M07_F10_KI16_7.mat', 'N15_M07_F10_KI16_5.mat', 'N15_M07_F10_KI16_4.mat', 'N15_M07_F10_KI16_9.mat', 'N15_M07_F10_KI16_8.mat', 'N15_M07_F10_KI16_20.mat', 'N15_M07_F10_KI16_18.mat', 'N15_M07_F10_KI16_19.mat']\n",
      "Y is:\n",
      "2\n",
      "filestring\n",
      "N15_M07_F10_\n",
      "l: \n",
      "[['KA22', '1500', '400', 'http://groups.uni-paderborn.de/kat/BearingDataCenter/KA22.rar']]\n",
      "file to exrtract is is::\n",
      "./data/PDB/KA22/raw/KA22.rar\n",
      "file already extracted, skipping unrar\n",
      "sorted filelist:\n",
      "['N15_M07_F10_KA22_9.mat', 'N15_M07_F10_KA22_8.mat', 'N15_M07_F10_KA22_20.mat', 'N15_M07_F10_KA22_19.mat', 'N15_M07_F10_KA22_18.mat', 'N15_M07_F10_KA22_16.mat', 'N15_M07_F10_KA22_17.mat', 'N15_M07_F10_KA22_15.mat', 'N15_M07_F10_KA22_14.mat', 'N15_M07_F10_KA22_10.mat', 'N15_M07_F10_KA22_11.mat', 'N15_M07_F10_KA22_13.mat', 'N15_M07_F10_KA22_12.mat', 'N15_M07_F10_KA22_5.mat', 'N15_M07_F10_KA22_4.mat', 'N15_M07_F10_KA22_6.mat', 'N15_M07_F10_KA22_7.mat', 'N15_M07_F10_KA22_3.mat', 'N15_M07_F10_KA22_2.mat', 'N15_M07_F10_KA22_1.mat']\n",
      "Y is:\n",
      "2\n",
      "filestring\n",
      "N15_M07_F10_\n",
      "l: \n",
      "[['KA16', '1500', '400', 'http://groups.uni-paderborn.de/kat/BearingDataCenter/KA16.rar']]\n",
      "file to exrtract is is::\n",
      "./data/PDB/KA16/raw/KA16.rar\n",
      "file already extracted, skipping unrar\n",
      "sorted filelist:\n",
      "['N15_M07_F10_KA16_6.mat', 'N15_M07_F10_KA16_7.mat', 'N15_M07_F10_KA16_18.mat', 'N15_M07_F10_KA16_5.mat', 'N15_M07_F10_KA16_4.mat', 'N15_M07_F10_KA16_19.mat', 'N15_M07_F10_KA16_1.mat', 'N15_M07_F10_KA16_20.mat', 'N15_M07_F10_KA16_3.mat', 'N15_M07_F10_KA16_2.mat', 'N15_M07_F10_KA16_12.mat', 'N15_M07_F10_KA16_13.mat', 'N15_M07_F10_KA16_11.mat', 'N15_M07_F10_KA16_10.mat', 'N15_M07_F10_KA16_14.mat', 'N15_M07_F10_KA16_9.mat', 'N15_M07_F10_KA16_8.mat', 'N15_M07_F10_KA16_15.mat', 'N15_M07_F10_KA16_17.mat', 'N15_M07_F10_KA16_16.mat']\n"
     ]
    }
   ],
   "source": [
    "l = 4*65000\n",
    "#healthy sets\n",
    "OK_sound_1 = PDB('K002','1500','1000','700',l) #\n",
    "OK_sound_2  = PDB('K001','1500','1000','700',l)\n",
    "\n",
    "IR_sound_1  = PDB('KI17','1500','1000','700',l) #severity 1\n",
    "IR_sound_2  = PDB('KI16','1500','1000','700',l) #severity 3\n",
    "\n",
    "OR_sound_1  = PDB('KA22','1500','1000','700',l) #severity 1\n",
    "OR_sound_2  = PDB('KA16','1500','1000','700',l) #severity 2\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(520000,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sound = OR_sound_2.X_train\n",
    "#sound = IR_sound_2.X_train\n",
    "\n",
    "np.shape(sound)\n",
    "# duplicate length = more sound\n",
    "s= np.append(sound,sound)\n",
    "np.shape(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sounddevice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "fs=64000\n",
    "#play sound at 44.1KHz ?\n",
    "#sd.play(sound[0,:],samplerate=fs)\n",
    "sd.play(s,samplerate=fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a pretrained model, test before running in EI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def download_file_from_google_drive(id, destination):\n",
    "    URL = \"https://drive.google.com/uc?export=download\"\n",
    "    session = requests.Session()\n",
    "\n",
    "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
    "    #response = session.get(URL, stream = True)\n",
    "\n",
    "    token = get_confirm_token(response)\n",
    "\n",
    "    if token:\n",
    "        params = { 'id' : id, 'confirm' : token }\n",
    "        response = session.get(URL, params = params, stream = True)\n",
    "\n",
    "    save_response_content(response, destination)    \n",
    "\n",
    "def get_confirm_token(response):\n",
    "    for key, value in response.cookies.items():\n",
    "        if key.startswith('download_warning'):\n",
    "            return value\n",
    "\n",
    "    return None\n",
    "\n",
    "def save_response_content(response, destination):\n",
    "    CHUNK_SIZE = 32768\n",
    "\n",
    "    with open(destination, \"wb\") as f:\n",
    "        for chunk in response.iter_content(CHUNK_SIZE):\n",
    "            if chunk: # filter out keep-alive new chunks\n",
    "                f.write(chunk)\n",
    "\n",
    "#https://drive.google.com/file/d/1q2LbTBRBhh4vXv3cfImEGJVODWenlsA2/view?usp=sharing\n",
    "#https://drive.google.com/file/d/1q2LbTBRBhh4vXv3cfImEGJVODWenlsA2/view?usp=sharing\n",
    "#https://drive.google.com/file/d/1q2LbTBRBhh4vXv3cfImEGJVODWenlsA2/view?usp=sharing\n",
    "file_id = '1q2LbTBRBhh4vXv3cfImEGJVODWenlsA2'\n",
    "destination = './model_tfl/m6.h5'\n",
    "download_file_from_google_drive(file_id, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/opprud/workspace/ceramicspeed/bearingbrain/tools/dataset_for_test/external_data/4_paderborn\n"
     ]
    }
   ],
   "source": [
    "!pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 5224\n",
      "drwxr-xr-x   9 opprud  staff      288 Oct  4 17:29 \u001b[34m.\u001b[m\u001b[m\n",
      "drwxr-xr-x  39 opprud  staff     1248 Oct  4 17:28 \u001b[34m..\u001b[m\u001b[m\n",
      "drwxr-xr-x   6 opprud  staff      192 Oct  4 17:28 \u001b[34m.ipynb_checkpoints\u001b[m\u001b[m\n",
      "-rw-r--r--   1 opprud  staff    61624 Oct  4 17:01 m\n",
      "-rw-r--r--   1 opprud  staff    61729 Oct  4 17:06 m2\n",
      "-rw-r--r--   1 opprud  staff    61837 Oct  4 17:08 m3\n",
      "-rw-r--r--   1 opprud  staff      145 Oct  4 17:27 m4\n",
      "-rw-r--r--   1 opprud  staff  1233048 Oct  4 17:28 m5\n",
      "-rw-r--r--   1 opprud  staff  1233048 Oct  4 17:29 m6.h5\n"
     ]
    }
   ],
   "source": [
    "!ls -al ./model_tfl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown regularizer: L2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-139-4d76f42e628c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load as Keras model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtfl_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./model_tfl/m6.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    182\u001b[0m     if (h5py is not None and (\n\u001b[1;32m    183\u001b[0m         isinstance(filepath, h5py.File) or h5py.is_hdf5(filepath))):\n\u001b[0;32m--> 184\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mhdf5_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model_from_hdf5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36mload_model_from_hdf5\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0mmodel_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     model = model_config_lib.model_from_config(model_config,\n\u001b[0;32m--> 178\u001b[0;31m                                                custom_objects=custom_objects)\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;31m# set weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/saving/model_config.py\u001b[0m in \u001b[0;36mmodel_from_config\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m     53\u001b[0m                     '`Sequential.from_config(config)`?')\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeserialize\u001b[0m  \u001b[0;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/serialization.py\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m    107\u001b[0m       \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m       \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m       printable_module_name='layer')\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    371\u001b[0m             custom_objects=dict(\n\u001b[1;32m    372\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_GLOBAL_CUSTOM_OBJECTS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m                 list(custom_objects.items())))\n\u001b[0m\u001b[1;32m    374\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mCustomObjectScope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer_config\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayer_configs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m       layer = layer_module.deserialize(layer_config,\n\u001b[0;32m--> 398\u001b[0;31m                                        custom_objects=custom_objects)\n\u001b[0m\u001b[1;32m    399\u001b[0m       \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m     if (not model.inputs and build_input_shape and\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/serialization.py\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m    107\u001b[0m       \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m       \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m       printable_module_name='layer')\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    373\u001b[0m                 list(custom_objects.items())))\n\u001b[1;32m    374\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mCustomObjectScope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m       \u001b[0;31m# Then `cls` may be a function returning a class.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config)\u001b[0m\n\u001b[1;32m    653\u001b[0m         \u001b[0mA\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m     \"\"\"\n\u001b[0;32m--> 655\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/convolutional.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filters, kernel_size, strides, padding, data_format, dilation_rate, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, **kwargs)\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0mkernel_initializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel_initializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0mbias_initializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_initializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m         \u001b[0mkernel_regularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mregularizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel_regularizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m         \u001b[0mbias_regularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mregularizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_regularizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m         \u001b[0mactivity_regularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mregularizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivity_regularizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/regularizers.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(identifier)\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[0midentifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/regularizers.py\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m    292\u001b[0m       \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m       \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m       printable_module_name='regularizer')\n\u001b[0m\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midentifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m     (cls, cls_config) = class_and_config_for_serialized_keras_object(\n\u001b[0;32m--> 362\u001b[0;31m         config, module_objects, custom_objects, printable_module_name)\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'from_config'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mclass_and_config_for_serialized_keras_object\u001b[0;34m(config, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    319\u001b[0m   \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_registered_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unknown '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mprintable_module_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m': '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mclass_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m   \u001b[0mcls_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'config'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown regularizer: L2"
     ]
    }
   ],
   "source": [
    "# Load as Keras model\n",
    "tfl_model = tf.keras.models.load_model('./model_tfl/m6.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfl_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-140-7bccbd7a4a35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtfl_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tfl_model' is not defined"
     ]
    }
   ],
   "source": [
    "tfl_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
